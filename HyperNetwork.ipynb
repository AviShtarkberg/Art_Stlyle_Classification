{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-07T17:53:03.386281Z",
     "start_time": "2025-01-07T17:53:03.379439Z"
    }
   },
   "source": [
    "# CELL 1: Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:53:05.444626Z",
     "start_time": "2025-01-07T17:53:04.934115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CELL 2: Data Setup & Transforms (Using your existing paths)\n",
    "dataset_root = r\"C:\\Users\\yozev\\OneDrive\\Desktop\\artFiltered\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "print(\"Classes:\", full_dataset.classes)"
   ],
   "id": "83daaa5e64519f45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Abstract_Expressionism', 'Art_Nouveau_Modern', 'Baroque', 'Cubism', 'Expressionism', 'Impressionism', 'Naive_Art_Primitivism', 'Northern_Renaissance', 'Post_Impressionism', 'Realism', 'Rococo', 'Romanticism', 'Symbolism']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:53:08.477632Z",
     "start_time": "2025-01-07T17:53:08.459085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CELL 3: Data Splitting\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.75 * dataset_size)\n",
    "val_size = int(0.10 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ],
   "id": "b6ec83693e5d6784",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:53:12.223845Z",
     "start_time": "2025-01-07T17:53:12.209988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, target_hidden_dim):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.weight_generator = nn.Sequential(\n",
    "            nn.Linear(256, 2048 * 512),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.bias_generator = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        embedded = self.embedding(z)\n",
    "        weights = self.weight_generator(embedded)\n",
    "        weights = weights.view(2048, 512)\n",
    "        biases = self.bias_generator(embedded)\n",
    "        return weights, biases\n",
    "\n",
    "class HyperResNet(nn.Module):\n",
    "    def __init__(self, num_classes, z_dim=64):\n",
    "        super(HyperResNet, self).__init__()\n",
    "\n",
    "        self.resnet = models.resnet50(weights='IMAGENET1K_V2')\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "        # Fixed dimensions for clarity\n",
    "        self.input_dim = 2048  # ResNet50 output dimension\n",
    "        self.hidden_dim = 512  # Hidden dimension\n",
    "\n",
    "        self.hyper_net = HyperNetwork(z_dim, self.hidden_dim)\n",
    "        self.z_dim = z_dim\n",
    "        self.z = nn.Parameter(torch.randn(z_dim))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get 2048-dimensional features from ResNet\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)  # Batch x 2048\n",
    "\n",
    "        # Generate weights (2048 x 512) and biases (512)\n",
    "        weights, biases = self.hyper_net(self.z)\n",
    "\n",
    "        # Matrix multiplication: (Batch x 2048) @ (2048 x 512) = (Batch x 512)\n",
    "        x = torch.matmul(features, weights) + biases\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "id": "caa3c3e14d6481d0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:53:18.114127Z",
     "start_time": "2025-01-07T17:53:16.703256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CELL 5: Model Setup and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(full_dataset.classes)\n",
    "model = HyperResNet(num_classes=num_classes)  # Make sure to instantiate the model\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create parameter groups, excluding z from the main parameters\n",
    "base_params = [p for n, p in model.named_parameters() if n != 'z']\n",
    "optimizer = optim.Adam([\n",
    "    {'params': base_params, 'lr': 1e-4},\n",
    "    {'params': [model.z], 'lr': 1e-3}\n",
    "])\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "49260ea6fc657913",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yozev\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T19:12:36.424162Z",
     "start_time": "2025-01-05T14:53:19.921186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    print(\"Training started...\")\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{epochs}] Training')\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_bar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            train_bar.set_postfix({\n",
    "                'batch': f'{batch_idx+1}/{len(train_loader)}',\n",
    "                'train_loss': f'{loss.item():.4f}'\n",
    "            })\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f'Epoch [{epoch+1}/{epochs}] Validation')\n",
    "            for images, labels in val_bar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # Metrics\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        # Save best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'z_vector': model.z.data,\n",
    "                'best_accuracy': best_accuracy,\n",
    "            }, os.path.join(dataset_root, 'best_hyper_model.pth'))\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"\\nEpoch [{epoch+1}/{epochs}] Summary:\")\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Validation Precision: {precision:.4f}\")\n",
    "        print(f\"Validation Recall: {recall:.4f}\")\n",
    "        print(f\"Best Accuracy: {best_accuracy:.2f}%\")\n",
    "        print('-'*50)\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted. Saving the current model...\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'z_vector': model.z.data,\n",
    "        'best_accuracy': best_accuracy,\n",
    "    }, os.path.join(dataset_root, 'interrupted_model.pth'))\n",
    "    print(\"Model saved as 'interrupted_model.pth'. You can resume or test it later.\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.2f}%\")\n"
   ],
   "id": "8afa34d0ea35e0f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Training: 100%|██████████| 1518/1518 [2:32:39<00:00,  6.03s/it, batch=1518/1518, train_loss=0.9664]\n",
      "Epoch [1/20] Validation: 100%|██████████| 203/203 [01:27<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20] Summary:\n",
      "Training Loss: 1.3295\n",
      "Validation Loss: 1.0151\n",
      "Validation Accuracy: 65.68%\n",
      "Validation Precision: 0.6692\n",
      "Validation Recall: 0.6568\n",
      "Best Accuracy: 65.68%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Training: 100%|██████████| 1518/1518 [2:31:58<00:00,  6.01s/it, batch=1518/1518, train_loss=0.9015]\n",
      "Epoch [2/20] Validation: 100%|██████████| 203/203 [01:25<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/20] Summary:\n",
      "Training Loss: 0.8020\n",
      "Validation Loss: 0.9324\n",
      "Validation Accuracy: 68.57%\n",
      "Validation Precision: 0.7017\n",
      "Validation Recall: 0.6857\n",
      "Best Accuracy: 68.57%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Training: 100%|██████████| 1518/1518 [2:32:10<00:00,  6.01s/it, batch=1518/1518, train_loss=0.3642]\n",
      "Epoch [3/20] Validation: 100%|██████████| 203/203 [01:26<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/20] Summary:\n",
      "Training Loss: 0.5283\n",
      "Validation Loss: 0.9890\n",
      "Validation Accuracy: 69.81%\n",
      "Validation Precision: 0.6987\n",
      "Validation Recall: 0.6981\n",
      "Best Accuracy: 69.81%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Training: 100%|██████████| 1518/1518 [2:32:35<00:00,  6.03s/it, batch=1518/1518, train_loss=0.2916]\n",
      "Epoch [4/20] Validation: 100%|██████████| 203/203 [01:25<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/20] Summary:\n",
      "Training Loss: 0.3431\n",
      "Validation Loss: 1.1484\n",
      "Validation Accuracy: 69.78%\n",
      "Validation Precision: 0.7091\n",
      "Validation Recall: 0.6978\n",
      "Best Accuracy: 69.81%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Training: 100%|██████████| 1518/1518 [2:32:12<00:00,  6.02s/it, batch=1518/1518, train_loss=0.1060]\n",
      "Epoch [5/20] Validation: 100%|██████████| 203/203 [01:25<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/20] Summary:\n",
      "Training Loss: 0.2400\n",
      "Validation Loss: 1.1959\n",
      "Validation Accuracy: 70.22%\n",
      "Validation Precision: 0.7053\n",
      "Validation Recall: 0.7022\n",
      "Best Accuracy: 70.22%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Training: 100%|██████████| 1518/1518 [2:32:06<00:00,  6.01s/it, batch=1518/1518, train_loss=0.5914]\n",
      "Epoch [6/20] Validation: 100%|██████████| 203/203 [01:25<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/20] Summary:\n",
      "Training Loss: 0.1904\n",
      "Validation Loss: 1.2323\n",
      "Validation Accuracy: 70.04%\n",
      "Validation Precision: 0.6997\n",
      "Validation Recall: 0.7004\n",
      "Best Accuracy: 70.22%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Training: 100%|██████████| 1518/1518 [2:32:28<00:00,  6.03s/it, batch=1518/1518, train_loss=0.0219]\n",
      "Epoch [7/20] Validation: 100%|██████████| 203/203 [01:25<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/20] Summary:\n",
      "Training Loss: 0.0821\n",
      "Validation Loss: 1.3284\n",
      "Validation Accuracy: 71.64%\n",
      "Validation Precision: 0.7168\n",
      "Validation Recall: 0.7164\n",
      "Best Accuracy: 71.64%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Training: 100%|██████████| 1518/1518 [2:32:29<00:00,  6.03s/it, batch=1518/1518, train_loss=0.0884]\n",
      "Epoch [8/20] Validation: 100%|██████████| 203/203 [01:24<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/20] Summary:\n",
      "Training Loss: 0.0441\n",
      "Validation Loss: 1.4324\n",
      "Validation Accuracy: 72.06%\n",
      "Validation Precision: 0.7197\n",
      "Validation Recall: 0.7206\n",
      "Best Accuracy: 72.06%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Training: 100%|██████████| 1518/1518 [2:32:21<00:00,  6.02s/it, batch=1518/1518, train_loss=0.0018]\n",
      "Epoch [9/20] Validation: 100%|██████████| 203/203 [01:25<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/20] Summary:\n",
      "Training Loss: 0.0318\n",
      "Validation Loss: 1.5184\n",
      "Validation Accuracy: 72.26%\n",
      "Validation Precision: 0.7208\n",
      "Validation Recall: 0.7226\n",
      "Best Accuracy: 72.26%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Training: 100%|██████████| 1518/1518 [2:32:17<00:00,  6.02s/it, batch=1518/1518, train_loss=0.0041]\n",
      "Epoch [10/20] Validation: 100%|██████████| 203/203 [01:25<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/20] Summary:\n",
      "Training Loss: 0.0286\n",
      "Validation Loss: 1.5207\n",
      "Validation Accuracy: 72.39%\n",
      "Validation Precision: 0.7250\n",
      "Validation Recall: 0.7239\n",
      "Best Accuracy: 72.39%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Training: 100%|██████████| 1518/1518 [2:33:05<00:00,  6.05s/it, batch=1518/1518, train_loss=0.0040]\n",
      "Epoch [11/20] Validation: 100%|██████████| 203/203 [01:26<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/20] Summary:\n",
      "Training Loss: 0.0189\n",
      "Validation Loss: 1.5783\n",
      "Validation Accuracy: 72.11%\n",
      "Validation Precision: 0.7220\n",
      "Validation Recall: 0.7211\n",
      "Best Accuracy: 72.39%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Training:   4%|▍         | 61/1518 [06:25<2:33:24,  6.32s/it, batch=61/1518, train_loss=0.0004]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 25\u001B[0m\n\u001B[0;32m     22\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     23\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 25\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     26\u001B[0m     train_bar\u001B[38;5;241m.\u001B[39mset_postfix({\n\u001B[0;32m     27\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_idx\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train_loader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     28\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     29\u001B[0m     })\n\u001B[0;32m     31\u001B[0m avg_train_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:53:59.545564Z",
     "start_time": "2025-01-07T17:53:27.244410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = torch.load(os.path.join(dataset_root, 'best_hyper_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")"
   ],
   "id": "cf14bac1883b298d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yozev\\AppData\\Local\\Temp\\ipykernel_41648\\1891230433.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(dataset_root, 'best_hyper_model.pth'))\n",
      "Testing: 100%|██████████| 304/304 [00:29<00:00, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results:\n",
      "Test Accuracy: 71.69%\n",
      "Test Precision: 0.7167\n",
      "Test Recall: 0.7169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
