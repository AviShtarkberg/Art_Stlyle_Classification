{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T12:01:39.114717Z",
     "start_time": "2025-01-01T12:01:35.455815Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T12:01:41.361075Z",
     "start_time": "2025-01-01T12:01:41.083618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 1) LOAD FEATURES AND LABELS\n",
    "# -------------------------------\n",
    "data = np.load(\"efficientnet_b0_features_pt.npz\")  # or .npz file from Keras code if needed\n",
    "X = data[\"features\"]  # Shape: (num_samples, 1280) for EfficientNet-B0\n",
    "y = data[\"labels\"]    # Shape: (num_samples,)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).long()  # Assuming multi-class integer labels"
   ],
   "id": "8f3c3a8b16dceb24",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T12:01:43.671671Z",
     "start_time": "2025-01-01T12:01:43.534520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------\n",
    "# 2) SPLIT DATA: 75% TRAIN, 10% VAL, 15% TEST\n",
    "# ---------------------------------------------------\n",
    "# First split: train vs. (val+test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_tensor,\n",
    "    y_tensor,\n",
    "    train_size=0.75,\n",
    "    shuffle=True,\n",
    "    stratify=y_tensor,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Among the remaining 25%, we want 10% val, 15% test (ratio 2:3).\n",
    "# That means test is 0.6 (3/5) of the temp set, val is 0.4 (2/5).\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.6,\n",
    "    shuffle=True,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")"
   ],
   "id": "27d77d97053300b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 48563, Val size: 6475, Test size: 9713\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T12:01:46.302158Z",
     "start_time": "2025-01-01T12:01:46.295821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------\n",
    "# 3) CREATE DATASET + DATALOADER CLASSES\n",
    "# ---------------------------------------------------\n",
    "class FeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple Dataset that wraps feature vectors and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = FeatureDataset(X_train, y_train)\n",
    "val_dataset   = FeatureDataset(X_val, y_val)\n",
    "test_dataset  = FeatureDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders (use mini-batches for training)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
   ],
   "id": "7fcba9efcb6ebdd2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T13:32:08.090795Z",
     "start_time": "2025-01-01T13:32:08.077087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------\n",
    "# 4) DEFINE A SIMPLE FEEDFORWARD NETWORK\n",
    "# ---------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeeperNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(DeeperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "num_features = X_train.shape[1]  # 1280 if using EfficientNet-B0\n",
    "num_classes  = len(torch.unique(y_train))\n",
    "\n",
    "model = DeeperNet(input_dim=num_features, hidden_dim=256, num_classes=num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function (CrossEntropy for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer (Adam, for example)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-5)"
   ],
   "id": "1d52f01cbcda2f82",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T13:37:30.768695Z",
     "start_time": "2025-01-01T13:32:09.710583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "epochs = 200  # Adjust as needed\n",
    "patience = 5  # Number of epochs to wait after last improvement in val loss\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Keep a copy of the best model weights\n",
    "best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # --------------------\n",
    "    # Training step\n",
    "    # --------------------\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Compute average train loss\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # --------------------\n",
    "    # Validation step\n",
    "    # --------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # --------------------\n",
    "    # Early Stopping Check\n",
    "    # --------------------\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        # Found an improvement, save the model weights\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        # No improvement, increase counter\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "# Restore the best weights\n",
    "model.load_state_dict(best_model_weights)\n",
    "print(f\"Training finished. Best validation loss: {best_val_loss:.4f}\")\n"
   ],
   "id": "49948d63554d1e65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Train Loss: 2.5636, Val Loss: 2.5616\n",
      "Epoch [2/200] Train Loss: 2.5599, Val Loss: 2.5575\n",
      "Epoch [3/200] Train Loss: 2.5551, Val Loss: 2.5519\n",
      "Epoch [4/200] Train Loss: 2.5481, Val Loss: 2.5431\n",
      "Epoch [5/200] Train Loss: 2.5363, Val Loss: 2.5274\n",
      "Epoch [6/200] Train Loss: 2.5140, Val Loss: 2.4961\n",
      "Epoch [7/200] Train Loss: 2.4664, Val Loss: 2.4265\n",
      "Epoch [8/200] Train Loss: 2.3642, Val Loss: 2.2904\n",
      "Epoch [9/200] Train Loss: 2.2150, Val Loss: 2.1429\n",
      "Epoch [10/200] Train Loss: 2.0815, Val Loss: 2.0271\n",
      "Epoch [11/200] Train Loss: 1.9809, Val Loss: 1.9436\n",
      "Epoch [12/200] Train Loss: 1.9080, Val Loss: 1.8834\n",
      "Epoch [13/200] Train Loss: 1.8512, Val Loss: 1.8332\n",
      "Epoch [14/200] Train Loss: 1.8024, Val Loss: 1.7904\n",
      "Epoch [15/200] Train Loss: 1.7596, Val Loss: 1.7483\n",
      "Epoch [16/200] Train Loss: 1.7214, Val Loss: 1.7136\n",
      "Epoch [17/200] Train Loss: 1.6882, Val Loss: 1.6829\n",
      "Epoch [18/200] Train Loss: 1.6593, Val Loss: 1.6597\n",
      "Epoch [19/200] Train Loss: 1.6336, Val Loss: 1.6352\n",
      "Epoch [20/200] Train Loss: 1.6105, Val Loss: 1.6164\n",
      "Epoch [21/200] Train Loss: 1.5890, Val Loss: 1.5971\n",
      "Epoch [22/200] Train Loss: 1.5689, Val Loss: 1.5798\n",
      "Epoch [23/200] Train Loss: 1.5496, Val Loss: 1.5641\n",
      "Epoch [24/200] Train Loss: 1.5306, Val Loss: 1.5456\n",
      "Epoch [25/200] Train Loss: 1.5122, Val Loss: 1.5301\n",
      "Epoch [26/200] Train Loss: 1.4940, Val Loss: 1.5145\n",
      "Epoch [27/200] Train Loss: 1.4766, Val Loss: 1.4987\n",
      "Epoch [28/200] Train Loss: 1.4595, Val Loss: 1.4831\n",
      "Epoch [29/200] Train Loss: 1.4427, Val Loss: 1.4698\n",
      "Epoch [30/200] Train Loss: 1.4271, Val Loss: 1.4554\n",
      "Epoch [31/200] Train Loss: 1.4108, Val Loss: 1.4412\n",
      "Epoch [32/200] Train Loss: 1.3958, Val Loss: 1.4315\n",
      "Epoch [33/200] Train Loss: 1.3807, Val Loss: 1.4191\n",
      "Epoch [34/200] Train Loss: 1.3668, Val Loss: 1.4066\n",
      "Epoch [35/200] Train Loss: 1.3529, Val Loss: 1.3965\n",
      "Epoch [36/200] Train Loss: 1.3398, Val Loss: 1.3876\n",
      "Epoch [37/200] Train Loss: 1.3282, Val Loss: 1.3764\n",
      "Epoch [38/200] Train Loss: 1.3163, Val Loss: 1.3709\n",
      "Epoch [39/200] Train Loss: 1.3049, Val Loss: 1.3624\n",
      "Epoch [40/200] Train Loss: 1.2941, Val Loss: 1.3540\n",
      "Epoch [41/200] Train Loss: 1.2830, Val Loss: 1.3501\n",
      "Epoch [42/200] Train Loss: 1.2726, Val Loss: 1.3431\n",
      "Epoch [43/200] Train Loss: 1.2628, Val Loss: 1.3345\n",
      "Epoch [44/200] Train Loss: 1.2532, Val Loss: 1.3285\n",
      "Epoch [45/200] Train Loss: 1.2434, Val Loss: 1.3221\n",
      "Epoch [46/200] Train Loss: 1.2341, Val Loss: 1.3189\n",
      "Epoch [47/200] Train Loss: 1.2256, Val Loss: 1.3126\n",
      "Epoch [48/200] Train Loss: 1.2164, Val Loss: 1.3108\n",
      "Epoch [49/200] Train Loss: 1.2073, Val Loss: 1.3023\n",
      "Epoch [50/200] Train Loss: 1.1984, Val Loss: 1.2973\n",
      "Epoch [51/200] Train Loss: 1.1904, Val Loss: 1.2909\n",
      "Epoch [52/200] Train Loss: 1.1821, Val Loss: 1.2912\n",
      "Epoch [53/200] Train Loss: 1.1737, Val Loss: 1.2842\n",
      "Epoch [54/200] Train Loss: 1.1658, Val Loss: 1.2806\n",
      "Epoch [55/200] Train Loss: 1.1573, Val Loss: 1.2778\n",
      "Epoch [56/200] Train Loss: 1.1492, Val Loss: 1.2738\n",
      "Epoch [57/200] Train Loss: 1.1413, Val Loss: 1.2705\n",
      "Epoch [58/200] Train Loss: 1.1336, Val Loss: 1.2675\n",
      "Epoch [59/200] Train Loss: 1.1264, Val Loss: 1.2609\n",
      "Epoch [60/200] Train Loss: 1.1180, Val Loss: 1.2605\n",
      "Epoch [61/200] Train Loss: 1.1099, Val Loss: 1.2558\n",
      "Epoch [62/200] Train Loss: 1.1025, Val Loss: 1.2514\n",
      "Epoch [63/200] Train Loss: 1.0949, Val Loss: 1.2501\n",
      "Epoch [64/200] Train Loss: 1.0873, Val Loss: 1.2446\n",
      "Epoch [65/200] Train Loss: 1.0792, Val Loss: 1.2448\n",
      "Epoch [66/200] Train Loss: 1.0719, Val Loss: 1.2436\n",
      "Epoch [67/200] Train Loss: 1.0641, Val Loss: 1.2401\n",
      "Epoch [68/200] Train Loss: 1.0565, Val Loss: 1.2414\n",
      "Epoch [69/200] Train Loss: 1.0493, Val Loss: 1.2296\n",
      "Epoch [70/200] Train Loss: 1.0414, Val Loss: 1.2318\n",
      "Epoch [71/200] Train Loss: 1.0339, Val Loss: 1.2281\n",
      "Epoch [72/200] Train Loss: 1.0260, Val Loss: 1.2249\n",
      "Epoch [73/200] Train Loss: 1.0191, Val Loss: 1.2236\n",
      "Epoch [74/200] Train Loss: 1.0105, Val Loss: 1.2169\n",
      "Epoch [75/200] Train Loss: 1.0025, Val Loss: 1.2172\n",
      "Epoch [76/200] Train Loss: 0.9948, Val Loss: 1.2185\n",
      "Epoch [77/200] Train Loss: 0.9874, Val Loss: 1.2120\n",
      "Epoch [78/200] Train Loss: 0.9794, Val Loss: 1.2106\n",
      "Epoch [79/200] Train Loss: 0.9722, Val Loss: 1.2122\n",
      "Epoch [80/200] Train Loss: 0.9637, Val Loss: 1.2078\n",
      "Epoch [81/200] Train Loss: 0.9562, Val Loss: 1.2062\n",
      "Epoch [82/200] Train Loss: 0.9480, Val Loss: 1.2056\n",
      "Epoch [83/200] Train Loss: 0.9402, Val Loss: 1.2014\n",
      "Epoch [84/200] Train Loss: 0.9320, Val Loss: 1.2031\n",
      "Epoch [85/200] Train Loss: 0.9248, Val Loss: 1.2024\n",
      "Epoch [86/200] Train Loss: 0.9162, Val Loss: 1.1984\n",
      "Epoch [87/200] Train Loss: 0.9081, Val Loss: 1.1935\n",
      "Epoch [88/200] Train Loss: 0.9003, Val Loss: 1.1975\n",
      "Epoch [89/200] Train Loss: 0.8920, Val Loss: 1.1918\n",
      "Epoch [90/200] Train Loss: 0.8843, Val Loss: 1.1911\n",
      "Epoch [91/200] Train Loss: 0.8764, Val Loss: 1.1920\n",
      "Epoch [92/200] Train Loss: 0.8674, Val Loss: 1.1884\n",
      "Epoch [93/200] Train Loss: 0.8600, Val Loss: 1.1880\n",
      "Epoch [94/200] Train Loss: 0.8509, Val Loss: 1.1871\n",
      "Epoch [95/200] Train Loss: 0.8428, Val Loss: 1.1831\n",
      "Epoch [96/200] Train Loss: 0.8345, Val Loss: 1.1869\n",
      "Epoch [97/200] Train Loss: 0.8265, Val Loss: 1.1902\n",
      "Epoch [98/200] Train Loss: 0.8178, Val Loss: 1.1836\n",
      "Epoch [99/200] Train Loss: 0.8093, Val Loss: 1.1856\n",
      "Epoch [100/200] Train Loss: 0.8002, Val Loss: 1.1825\n",
      "Epoch [101/200] Train Loss: 0.7924, Val Loss: 1.1871\n",
      "Epoch [102/200] Train Loss: 0.7832, Val Loss: 1.1879\n",
      "Epoch [103/200] Train Loss: 0.7751, Val Loss: 1.1857\n",
      "Epoch [104/200] Train Loss: 0.7668, Val Loss: 1.1835\n",
      "Epoch [105/200] Train Loss: 0.7582, Val Loss: 1.1920\n",
      "Early stopping triggered at epoch 105.\n",
      "Training finished. Best validation loss: 1.1825\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T14:41:52.880941Z",
     "start_time": "2025-01-13T14:41:52.335530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------\n",
    "# 6) EVALUATION ON TEST SET (METRICS)\n",
    "# ---------------------------------------------------\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        # For classification, the predicted class is argmax\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_labels.append(batch_y.cpu().numpy())\n",
    "\n",
    "# Flatten lists\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Accuracy, Precision, Recall (multi-class setting)\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "# \"weighted\" handles label imbalance for multi-class\n",
    "prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"=== TEST METRICS ===\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {prec:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")"
   ],
   "id": "c2cf0463727553a",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# ---------------------------------------------------\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# 6) EVALUATION ON TEST SET (METRICS)\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# ---------------------------------------------------\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m      5\u001B[0m all_preds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      6\u001B[0m all_labels \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f88171f26cae5b71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
